{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangle report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The purpose of this project is to asses, clean, and analyze Twitter data. We will be analyzing data from a channel called WeRateDogs (@dog_rates). WeRateDogs is a channel that rates dogs - the comments are funny and ratings are almost always above 10 which is the denominator.\n",
    "\n",
    "##### This report will describe what steps we took to wrangle the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project details\n",
    "\n",
    "##### We will divide our data wrangling into three parts:\n",
    "\n",
    "- Gathering data\n",
    "- Assessing data\n",
    "- Cleaning data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gathering data\n",
    "\n",
    "We gathered our data from three separate places:\n",
    "\n",
    "##### - Twitter archive\n",
    "> The Twitter archive was a file one hand which we downloaded manually\n",
    "\n",
    "##### - Tweet image predictions\n",
    "> What breed of dog is present in each tweet according to a neural network. This was a .tsv file that we needed to download programmatically from a cloud server.\n",
    "\n",
    "##### - Twitter API\n",
    "> Using 'tweet ID' and and WeRateDogs Twitter archive I queryed Twitter API for each tweet's JSON data. I used Tweepy library to store the data in 'tweet_json.txt'. Next I read the data line by line ('tweet_id', 'favorite_count', 'retweet_count', 'created_at', 'source', 'retweeted_status', 'url') into pandas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing data\n",
    "\n",
    "##### Once the data was uploaded to pandas / Jupyter Notebook I used two methods to assess the data:\n",
    "\n",
    "1. Visual assessment - I opened all data frames and checked visually for and inconsistencies. \n",
    "\n",
    "2. Programmatic assessment - By using many standart methods like\n",
    "- info(), columns, describe(), value_counts(), count(), sample(5), head()\n",
    "\n",
    "Based on this info I came up with the problems listed below:\n",
    "\n",
    "##### Quality:\n",
    ">- Align data types between data frame (tweet_id)\n",
    ">- Change dates to the same formats\n",
    ">- Drop unnecessary columns\n",
    ">- Keep only original tweets\n",
    ">- Correct or drop rating_denominator\n",
    ">- Correct or drop rating_numerator\n",
    ">- Drop rows without proper dog names\n",
    ">- Only keep predictions with the highest confidence level\n",
    ">- Correct prediction names\n",
    "\n",
    "#####  Tidiness:\n",
    ">- Merge Three datadrames into one using Tweet_id\n",
    ">- Merge Dog stages into one column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step I took was to change the type format for tweet_id for all data frames this way I was sure the merging the data will not produce any issues.\n",
    "\n",
    "Next, I merged the three data frames which helped me have a big picture of the whole data frame.\n",
    "\n",
    "After that, I removed columns which in my opinion were unnecessary. Moreover based on the analysis of 'Tweet image predictions' I removed p2 and p3 as they had the lowest confidence levels.\n",
    "\n",
    "Next, I cleaned the Name column by finding and dropping incorrect names - dropped all rows with lowercase names.\n",
    "\n",
    "The next step was to keep only the original tweets - I accomplished that by removing tweets that had 'retweeted_status_id'.\n",
    "\n",
    "Merging Dog stages into one column was next on my list. First I removed all the instances where there was a 'None'. Next, I merged to columns, here we had a problem as suggested by the reviewer - some records had multiple stages. In that case, we kept both stages and divided them with a comma. \n",
    "\n",
    "Next, I cleaned the 'rating_denominator' and 'rating_numerator', as suggested by the reviewer we extracted correct values for both columns from the text and I changed them to floats. Then, I checked for more discrepancies. I found that in multiple cases the rating was given for multiple dogs in one tweet in that case I divided the rating by the number of dogs and corrected it manually. Moreover, I corrected I did this both for 'rating_denominator' and 'rating_numerator'. After that, I dropped one tweet which in my opinion had an incorrect rating and there was no indication of what was the correct score. \n",
    "\n",
    "Next, I cleaned the 'prediction names' column using the str.replace method.\n",
    "\n",
    "The final step was to change the 'timestamp' datetime format. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final steps\n",
    "\n",
    "The finished data frame was saved to 'twitter_archive_final.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
